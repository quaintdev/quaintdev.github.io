[{"content":"Gone are the days when you could just find a random fun blog where someone wrote about tinkering with their home lab or someone shared about the obscure art they know. Today\u0026rsquo;s web is highly curated and everything is filtered out to show you the content that would generate most revenue. Search engines are filled with optimized results that can easily be categorized as spam. And social media is filled with highly polished, attractive content that may not add any value at all to an individuals life.\nThe web has become increasingly hostile place because of ads. Huge corporations are pouring billions to grab eyeballs on the web. The ads are becoming intrusive and there are third party trackers everywhere. Trackers that track your interests and show you ads depending on that. Using web without an adblocker is a nightmare. Now it\u0026rsquo;s getting difficult to block ads[1] as well. Then there\u0026rsquo;s case of AI generated content. It is only going to increase and it will become increasingly difficult to discern from real content.\nDitch The Feed I mostly hang out on Hacker News and Reddit. I don\u0026rsquo;t have any other social media and I have found myself worrying about global politics, climate change, microplastics and whole lot of other things. Things that I have no control over. Don\u0026rsquo;t get me wrong it\u0026rsquo;s good to be informed about these things but constant influx of such information is not helping anyone. It\u0026rsquo;s critical to regulate what we consume on web because our brains have a tendency to think about what we consume. Our very thoughts are the result of what information we are consuming. Seriously ditch the feed someone else is curating for you!\nCurate Your Own After all issues above one might want to ditch all the social media and web along with it but we all know that\u0026rsquo;s not possible. The best we can do is limit the influx of information and curate it such that it\u0026rsquo;s good for our well being. But how? Find people and organizations you can trust. There are bloggers and content creators who know their craft and audience. These are the once who prefer authenticity over anything else. You will never see AI generated content from these. Subscribe to such people and organizations. Add them to your feed or remove them if they aren\u0026rsquo;t adding any value to your life. Curate your own feed religiously.\nBut how and where? Enters RSS.\nWhat is RSS Like I mentioned previously it\u0026rsquo;s like a social media feed but you are the curator. You choose sources that will appear in your feed and not some algorithm which is prioritizing revenue of some corporation over your interests. There are no trackers or intrusive ads in RSS. You can configure how frequently you want to refresh the feed. You choose the application to browse the feed. Unlike web where you only have Chrome or Firefox for browsing, RSS feeds can be read on any of 1000s of clients available[2]. Choose one that suits you. You can even use browser to read your feed.\nA Word of Advice: limit the number of sources and frequency of refresh. That way you will have the best possible experience browsing the web. Once you become accustomed to RSS you can configure it to consume everything from blogs to YouTube videos and even podcasts.\nOne more thing, sometimes websites do not provide RSS feed and in that case there\u0026rsquo;s nothing we can do except request them to provide one. But such sites are limited and most websites will always provide RSS feed. Just look for icon below. You can copy URL of image and paste it into your feed to subscribe to this blogs RSS feed!\nNotes:\nGoogle Chrome has started warning users that their ad blockers may soon be disabled\nI personally use miniflux but it needs to be self hosted so I\u0026rsquo;m suggesting Feed flow which is open source RSS reader and available for both Android and iOS.\n","permalink":"https://rohanrd.xyz/posts/why-rss/","summary":"Gone are the days when you could just find a random fun blog where someone wrote about tinkering with their home lab or someone shared about the obscure art they know. Today\u0026rsquo;s web is highly curated and everything is filtered out to show you the content that would generate most revenue. Search engines are filled with optimized results that can easily be categorized as spam. And social media is filled with highly polished, attractive content that may not add any value at all to an individuals life.","title":"Why RSS"},{"content":"So long Google There was a time I thought I would never switch away from Google. But times change, things fall apart. An incredible search engine starts giving you answers for the questions you never asked. A search engines job is to find best results for given keywords and then allow the user to navigate away from them. With Google, they are choosing the result for me. They are showing way too much information that I\u0026rsquo;m not really looking for. Try searching for a city and you will know what I\u0026rsquo;m talking about.\nThey are tuning search such that it becomes more accessible to people who like asking questions and getting answers. There\u0026rsquo;s nothing wrong with this. The sweet analytics that they collect must have been telling them that large number of people like to ask questions and are expecting answers. So that\u0026rsquo;s the direction they have chosen for search. This is normal progression of any Software when they choose to follow analytics. Of course with their popularity they could have chosen to ignore analytics and make search better. Everyone would eventually learn the art of searching but that\u0026rsquo;s just my idealism.\nI prefer good old searching with keywords instead of asking questions. When I\u0026rsquo;m typing those keywords I want the search engine to find all the pages across the web that might have those keywords. Google still does that but there focus has shifted from returning best results to keeping users engaged on the search page. AI generated answers are now front and center. Then there\u0026rsquo;s that annoying questions panel that keeps expanding with more questions if you click one of the answers. Search results are below all these panels. This shows their priorities. Instead of all this AI fluff they could have chosen to solve the SEO spam issue but no.\nAnyways, my search experience has siginificantly deviated from what it was few years ago. I don\u0026rsquo;t think they can ever go back on this. New and existing users now expect everything that they have been doing on search page. People like me who prefer the good old search will just have to find new search engine.\nHello Ecosia I found out about Ecosia from a comment on Hacker News. It stated that the search engine plants trees when you search. My first thought was that must be some poor search engine which plants trees just to create headlines. But I was wrong. Few days later, I was annoyed with Google and decided to try Ecosia. It\u0026rsquo;s been a long time that I used a search engine other than Google and I was not disappointed with the results. The best way to try out a search engine is to change your deafult search in your browser and use it for few days. I have been using Ecosia for a month now and I\u0026rsquo;m happy to say that I love it. I don\u0026rsquo;t see myself switching back to Google.\nThe search engine is clean. It does not have any dynamic panels that expand at will. Search results are front and center and AI is hidden away in separate tab. They do show occasional information panels like currency converter or the dictionary panel but these panels are limited and are useful when shown. Searching on Ecosia reminds you of old days of Google. You can\u0026rsquo;t sign in to Ecosia. They don\u0026rsquo;t store any history and is good from user privacy perspective. You can disable the cookies you don\u0026rsquo;t need from their settings page.\nWith Ecosia it\u0026rsquo;s not just better search experience, 100% of their profits go to climate action. They really do plant trees when we are searching! All the information regarding their financials is easily accessible from homepage and they are very transparent about their spending. I actually disabled my adblocker for Ecosia. These ads help them make profits which finally go to climate action. It\u0026rsquo;s refreshing to see a company using their profits for better tomorrow.\nIf you are excited to try it out, here you go: https://www.ecosia.org\nSubscribe to RSS feed for future updates!\nP.S. If you are someone who prefers Google but don\u0026rsquo;t like the new changes that are happening on results page then you can still get old results only page by using the \u0026ldquo;Web\u0026rdquo; filter from \u0026ldquo;More\u0026rdquo; dropdown. With this filter, Google will not show any AI answers or suggest any questions. I tried this for few days but the way this feature is hidden away shows Google\u0026rsquo;s priorities.\n","permalink":"https://rohanrd.xyz/posts/goodbye-google.-hello-ecosia/","summary":"So long Google There was a time I thought I would never switch away from Google. But times change, things fall apart. An incredible search engine starts giving you answers for the questions you never asked. A search engines job is to find best results for given keywords and then allow the user to navigate away from them. With Google, they are choosing the result for me. They are showing way too much information that I\u0026rsquo;m not really looking for.","title":"Goodbye Google. Hello Ecosia!"},{"content":"Back in 2009, anynone with a Nokia could have a personal website running on their own phone. You could host your own blog or share photos directly from your camera roll. People could message you or even send you sms over HTTP. There was other functionality but you get the gist. Sadly this amazing piece of tech was never widely adopted. Today\u0026rsquo;s phones are far more powerful than those Nokias both in performance and battery backup and still we don\u0026rsquo;t see anyone running a server on their phone. Why?\nI think this is because doing something like this that would make web more open was never a priority for large corporations.Think about it there\u0026rsquo;s no incentive for them to work on something like this. In fact, it might be something they want to actively avoid in order for their walled gardens to flourish. Then there are some tech aspects too that need to be worked on. A large number of population is behind CG-NAT and hence can\u0026rsquo;t use mobile server like this. Although this is changing with IPv6. Then there\u0026rsquo;s Android\u0026rsquo;s case of not allowing any application to bind below 1000 port. This prevents running web server on traditional 80 or 443 port.\nToday\u0026rsquo;s phone can definitely handle a small personal static website that has similar functionality like that of original Nokia web server. The reason I think this is needed is because a large percent of Internet users cannot afford hosting personal websites or have no idea how to. If we made setting up a website as easy as few clicks a lot of people would want to create their own websites.\nEarly Internet users had the privilege of self hosting from home, at that time there was no CG-NAT. Today\u0026rsquo;s users never got to experience that possibility. Most of current population joined Internet when their devices were already behind CG-NAT. Maybe the next billion websites can come from average phone users but someone has to provide the software to do it easily. All we need is to package a server like caddy for Android/iOS and provide IPv6 connectivity.\n","permalink":"https://rohanrd.xyz/posts/every-phone-should-be-able-to-run-personal-website/","summary":"Back in 2009, anynone with a Nokia could have a personal website running on their own phone. You could host your own blog or share photos directly from your camera roll. People could message you or even send you sms over HTTP. There was other functionality but you get the gist. Sadly this amazing piece of tech was never widely adopted. Today\u0026rsquo;s phones are far more powerful than those Nokias both in performance and battery backup and still we don\u0026rsquo;t see anyone running a server on their phone.","title":"Every Phone Should Be Able to Run Personal Website"},{"content":"We discussed regarding the hardware that I use for self hosting here. Let\u0026rsquo;s prepare our Pi for selfhosting\nInstall Raspbian Downloads\nRaspberry Pi Imager is the quick and easy way to install Raspberry Pi OS and other operating systems to a microSD card, ready to use with your Raspberry Pi Download Raspbian Lite. Choose 64 bit version Once you have downloaded these, start imager tool. Now choose \u0026ldquo;Use custom\u0026rdquo; option and specify the OS image location. Next you have to choose the storage option which will list the memory card that you will be inserting in Raspberry Pi. Once you have set these, select the gear icon to specify initial preferences for the OS. specify below options and check the set hostname box. You can also set the locale. I am assuming you will be using Ethernet connection. But if you want to use WiFi you should set those preferences as well.\nWrite the image, it will take some time and will give a message when you can eject the memory card. You can now insert it in the memory card slot on your Pi. At this point you are ready to power on your Pi. Once it\u0026rsquo;s powered on, use app like Wifiman on your phone to determine IP address of your Pi. You can log into the shell of your Pi using ssh\nssh pi@192.168.1.12\nIt will prompt you for the password that we set earlier. Your Pi is ready to self host but let\u0026rsquo;s do some housekeeping before we install any applications.\nInstall Log2ram Log2ram is required because we don\u0026rsquo;t want to write logs generated by our applications to the memory card. Instead we store those logs in RAM. This is mostly useful for preventing wear and tear of our SD card. Run below commands to install log2ram.\necho \u0026#34;deb [signed-by=/usr/share/keyrings/azlux-archive-keyring.gpg] http://packages.azlux.fr/debian/ bookworm main\u0026#34; | sudo tee /etc/apt/sources.list.d/azlux.list sudo wget -O /usr/share/keyrings/azlux-archive-keyring.gpg https://azlux.fr/repo.gpg sudo apt update sudo apt install log2ram After installation you need to reboot. You can check if it\u0026rsquo;s working by checking log2ram service status or by running below command\nsudo df -h | grep log2ram More details on installation and troubleshooting here\nInstall Podman Podman is a tool for managing OCI containers and pods. Don\u0026rsquo;t worry about what it is and what it does. You will have your answers as soon as you start using it. You can use below command to install podman\nsudo apt install podman\nNow run podman ps it\u0026rsquo;s output should be clean one liner. If you see error relate to cgroup do the following:\nCreate directory mkdir -p ~/.config/containers Add a file containers.conf in the directory created above using below command cat \u0026gt; ~/.config/containers/containers.conf [engine] events_logger = \u0026#34;file\u0026#34; cgroup_manager = \u0026#34;cgroupfs\u0026#34; Run the podman ps command again and the error related to cgroup should be gone. Mount External Drives Our apps and data will stay on external drives since we want to prevent wear and tear of memory card. Follow these steps to mount external drive:\nYour external drive should be visible in output of command sudo blkid Note the UUID corresponding to your drive from above command Create a folder like /mnt/avalon where we will mount our external drive. Change permissions or ownership of this directory as required. Make backup of file /etc/fstab and add below line at the end of file. Update your UUID and mount location and keeping everything else same.1 UUID=f8cd0001-1328-4f6a-80fa-8c7b7dea09a4 /mnt/avalon ext4 defaults,auto,users,rw,nofail,exec 0 0 Use mount -av to reload changes made to /etc/fstab You Raspberry Pi is now ready for installing self hosted applications.\nI am assuming you have ext4 drive. If your drive is formatted as some different filesystem you will have to update fstab accordingly. Here\u0026rsquo;s a good reference\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://rohanrd.xyz/posts/first-steps-after-installing-raspbian/","summary":"We discussed regarding the hardware that I use for self hosting here. Let\u0026rsquo;s prepare our Pi for selfhosting\nInstall Raspbian Downloads\nRaspberry Pi Imager is the quick and easy way to install Raspberry Pi OS and other operating systems to a microSD card, ready to use with your Raspberry Pi Download Raspbian Lite. Choose 64 bit version Once you have downloaded these, start imager tool. Now choose \u0026ldquo;Use custom\u0026rdquo; option and specify the OS image location.","title":"First Steps After Installing Raspbian"},{"content":" Imagine our ancestors, who were primarily hunter-gatherers, when faced with predator like a Lion thought \u0026ldquo;This lion will not eat me\u0026rdquo;. Or consider example of the 2008 financial crisis, people thought \u0026ldquo;mortgage backed securities will never go down\u0026rdquo;. Then every now and then throughout the world their are banks that collapsed and eventually had to be bailed out by government. In each of these examples, people failed to consider a perspective. These people were not dumb but they were blinded by what I call toxic positivity. I see so many instances where the situation was dire and people were being positive about it. All it does is exacerbates the problem.\nNow consider the flip side, which I think was how our hunter-gatherer ancestors would have solved the problem. Instead of relying on Lion not eating them. They controlled what they could, expecting the worst. They started living and hunting in groups which gave them obvious advantage. They did this until man became the apex predator in the food chain. If they had not done that we would not be here seeing this day.1 Same goes for the financial crisis, if more people had considered a negative perspective2 that financial crisis could have been avoided. This negative perspective is what I call thoughtful negativity.\nThoughtful negativity is better than toxic positivity\nIn conclusion what I want to say is negative perspective is not bad. Perception of reality is always subjective. If we consider the negative perspective along with positive, I think, we get a more clearer picture of reality.\nReminds me of a scene from The Leftovers season 2. It\u0026rsquo;s opening scene and is very beautiful depiction of how our human race has evolved. I will not spoil that scene for you. Go watch it, it\u0026rsquo;s worth watching.\u0026#160;\u0026#x21a9;\u0026#xfe0e;\nFew people did considered the negative perspective and earned quite some money. You can watch the movie Big Short or related documentaries if you haven\u0026rsquo;t already\u0026#160;\u0026#x21a9;\u0026#xfe0e;\n","permalink":"https://rohanrd.xyz/posts/toxic-positivity/","summary":"Imagine our ancestors, who were primarily hunter-gatherers, when faced with predator like a Lion thought \u0026ldquo;This lion will not eat me\u0026rdquo;. Or consider example of the 2008 financial crisis, people thought \u0026ldquo;mortgage backed securities will never go down\u0026rdquo;. Then every now and then throughout the world their are banks that collapsed and eventually had to be bailed out by government. In each of these examples, people failed to consider a perspective.","title":"Toxic Positivity"},{"content":"If you have been following the Twitter drama over last month, you must have heard about Mastodon. Ten\u0026rsquo;s of thousands of people are leaving Twitter for Mastodon. What is Mastdon? Originally it\u0026rsquo;s an extinct animal but a few years ago Eugen Rochko created a new social network called Mastodon. This is not just any social network, it\u0026rsquo;s a federated. What do I mean by that?\nConsider this, you have Instagram account but you want to follow someone on Twitter, now you know this is not possible with our conventional social networks. Not that this is technically impossible as you will see in some time. This is not possible frankly because it is not conducive to growth of these giant companies. You see, the primary stream of income for these social networks is Ad revenue and that will grow only if users spend more time on their app or website. If they implemented something that will allow interoperability then they will be shooting themselves in the foot as they will no longer be able to hold their own users captive by employing questionable algorithms and dark UI patterns. Also this will allow small companies or someone like you and me to start our own social network and let their users interact with our social network. There social network will not be the walled garden that they so desperately want it to be.\nThis all sounds bleak but soon things are going to change because I believe people eventually choose freedom and power of choice over convenience. This is exactly the things that a new protocol called ActivityPub enables. Any social network that implements this protocol will make that network compatible with other social networks who implemented the protocol. I believe these decentralized networks are set to take over conventional social media over next few decades. You might ask why I\u0026rsquo;m so sure of it. Imagine telling the idea of democracy a few centuries back. And that is exactly the idea of these new social network sounds like to conventional social media users today.\nMastodon is the first social network to support ActivityPub. It\u0026rsquo;s similar to Twitter but is far superior to it in terms of features. Pixelfed is another social network that supports ActivityPub and is similar to Instagram. Needless to say these networks allow you to follow people from one another. There are many networks that are already supporting ActivityPub and you can find out about them here\nLike I meantioned earlier you and me can host our own Mastodon instance that can federate with other networks. Because there are 1000s of servers spread across the globe each with it\u0026rsquo;s own rules, it becomes easy to manage and moderate these social networks. Like minded individuals can start their own server to discuss topics of their interest, some of these are listed here. Moreover the decentralization helps in blocking bad actors that are engaging in harmful behaviours. All servers are run voluntarily by few community members, which means there are no ads! I think this model of social media reflects our societies in general. There\u0026rsquo;s only one con that I see at the moment and that is your experience depends on the server on which you have signed up but once you figured that out you are ready to go. I highly recommend you check out fedi.tips. If you want to join one of these networks, I would recommend https://toot.community for something like Twitter and https://pixelfed.social for something like Instagram. Of course you can follow people other network once you join one of them. It\u0026rsquo;s just PixelFed is more suitable if you prefer pictures over text.\n","permalink":"https://rohanrd.xyz/posts/social-media-and-decentralization/","summary":"How decentralization is going to change social media","title":"Social Media and Decentralization"},{"content":" People in tech circles are increasingly moving towards self hosting and I believe this trend is going to catch on with general population as both hardware and Software continues to evolve. This evolution has already reached to a point where a layman with few hours to spare can self host alernative to most of popular cloud based applications. In this blog post I am going to show you the hardware that I have used for low cost server setup at my home.\nThe Server You can use old PC/laptop as server for your home in which case you don\u0026rsquo;t even have to buy anything but I would recommend that you use a Raspberry Pi instead. There are quite a few advantages if you go this route.\nThey are fast and low on power consumption. You can stream 4K content with Raspberry Pi 4. They are cheap. 4GB version will cost you around 5000 INR without accessories. We want our server to run 24x71 and the power consumption adds up over time so it is an important factor while choosing the hardware for our server. A Raspberry Pi will cost you once in beginning but its going to save you lot of money in the long run. Here\u0026rsquo;s a calculator to calculate power consumption of any appliance. 10W is average power consumption of Pi. Try putting 65W on that calculator and see how numbers change.\nStorage If you are just beginning your self hosting journey I would advise you to start with a Pen drive. This way you can experiment while keeping costs down. If you are determined to have long term self hosting solution then read on.\nThe storage you choose depends on what you want to use your server for. For me, I want to host a Jellyfin media server that can playback 4K content. This requires that I have at least a Tera byte of storage. Note that Raspberry Pi runs of a memory card and you don\u0026rsquo;t need anything else but I would suggest to use that only for booting the Pi. Any data that you want to store for long term should never end up on that memory card. Always use external storage like a pen drive or HDD with Pi. suitable for its booting but not for any long term storage.\nFew considerations while choosing storage hardware\nPi should be able to serve 4K content. This requires that the HDD be connected over USB3.0. The drive should go to sleep when not in use. This reduces power consumption as well as wear and tear of the drive thus increasing the life span. Our storage solution should have proper heat dissipation. You can connect an external HDD to your Pi and be done with it but there are 2 disavantages to this approach. First, xxternal HDDs have poor heat dissipation which is okay considering they are built for one off usage and not running a mediacenter of them. They are certainly not designed to run 24x7. Second, external HDDs usually have 2.5inch form factor which limits their speed to 5400RPM. These are basically same hard disks as laptops which are slow as compared to internal hard disks used in desktops\nAfter careful consideration of many HDD connectivity solutions I came to conclusion that PiBox dock is best hardware for our low cost server. It can suspend the drive when not in use. It supports both 3.5/2.5 inch HDD and has USB3.0 interface. I have chosen Seagate Barracuda 3.5 inch HDD for my requirement. Another advantage of going with internal HDD is that they are cheaper as compared to their external counterpart. Regarding heat dissipation, since our drive is internal and docked there is no need for additional heat dissipation. I have checked temperatures and they are well within operating range of HDD. Ideally we should be using an enclosure with fans but that goes way over the budget for the low cost server that we want to build.\nBill of Materials:\nRaspberry PI 4 with case and fan - INR 5800 PiBox HDD Dock - INR 1100 Seagate Barracuda HDD 1TB - INR 3000 It costs little below INR 10,000 but if you compared this to the NAS solutions that are available in market this is pretty low cost. Not to mention its robust as well. I have been streaming 4K content, hosting an alternative to Google Photos and also a document server capable of running OCR on incoming documents but Raspberry Pi hardly breaks a sweat2. It\u0026rsquo;s a tiny little beast. In my next post I will show you the kind of applications that I am self hosting at home. So stay tuned.\nNotes:\nWe can turn off our server every night further improving cost savings but I will leave that as an exercise for the reader Your Pi will have hard time if you are expecting it to perform OCR while also transcoding media streams for multiple users. So yes your apps need to be configured properly to make the best out of this low cost setup. ","permalink":"https://rohanrd.xyz/posts/getting-started-with-self-hosting/","summary":"Get started with your self hosting journey","title":"Getting Started With Self Hosting"},{"content":"Remember Picasa? There is no native application even today that comes close to it\u0026rsquo;s feature parity. But sadly it was discontinued in favour of a cloud application: Google Photos. What happened with Picasa also happened to millions of other applications around the same time Picasa was discontinued. The applications we use to run on our PC have moved to cloud. It makes sense from corporations perspective because they get more control over updates, user data and licenses but users are at huge disadavantage as they no longer are in control of their own data. You might ask why that is such an issue? Let\u0026rsquo;s see:\nConsider this, you carefully curate your playlists on Spotify but every now and then you see a certain song missing from your playlist. Same goes for videos saved in your YouTube playlists or other music/video streaming services. Then there is also the case of OTT streaming platforms where the show you were going to watch over weekend has now disappeared. If this was not bad imagine what happens when you need document from your email but the provider has suspended your account because of something you did (which was not even violation of terms of service) on other service of that email provider. Hacker News is littered with such stories. There are some stories where the user never figured out what they had done wrong to deserve suspension of account.\nThen there is this whole different problem of corporations tracking you and your data. Whenever I bring this up people are like \u0026ldquo;I don\u0026rsquo;t care, I have nothing to hide\u0026rdquo;. But this is exactly similiar to saying \u0026ldquo;I don\u0026rsquo;t care about free speech because I have nothing to say\u0026rdquo;. Would you give up your right to free speech because you have nothing to say? No. The same goes about right to privacy. The data you share is used to cater you personalized ads. That same data is also used to ensure that you spend maximum time with the particular service. There are millions of engineers across the world who are getting paid hefty sum to keep as many eyeballs as possible glued to service offered by their employers. This engineering talent is supposed to be solving world\u0026rsquo;s problems but instead they are ensuring how everyone wastes their time. That\u0026rsquo;s a whole different topic for another blog post.\nWhat self hosting does is it protects you against all these issues and more. It gives you the peace of mind by keeping you in control of your data. You can choose if you never want your data to leave your premises or home. You can also choose to make your data available to you whenever or wherever you want it securely. In upcoming posts, we will explore self hosted applications and how they protect you and your data. But if you cannot wait, head over to r/selfhosted or check out this awesome self hosted list to get started with your self hosting journey.\nSubscribe to RSS feed to stay updated.\n","permalink":"https://rohanrd.xyz/posts/why-start-self-hosting/","summary":"Take control back of your digital life","title":"Why Start Self Hosting"},{"content":" India is rapidly switching to renewable energy sources. It\u0026rsquo;s not just the government that\u0026rsquo;s making the switch but people as well have been switching to solar roof tops for their power requirements. In our society, 30-40% houses have set up solar panels on their roofs. Most of these houses do not pay at all to the MSEB but on the other hand contribute surplus to the grid.\nThe one that is setup at my home - shown in picture above - has 2.27 kWh capacity. Over the past 3-4 months I have been monitoring the output of these panels and they have been on average producing 220-270 kW per month. There is around 70-100 units of surplus which is fed into the grid every month. I have seen output of about maximum 11.6 kWh in a single day. Rest of days it fluctuates in 6-8 kWh range. There are many parameters affecting output like weather, panel temperature, dust on the panels. I do not have any control over the weather but rest two parameters can be kept in check to improve efficiency of these panels.\nCurrently, only way to monitor output of these panels is by checking the inverter manually. This is not helpful at all. Moreover I would prefer to have this data recorded on per minute basis rather than looking at aggregates at the end of the day. I need a system that will continuously log the panel output to a central database. Logging to central database has its own advantages: I can not only do a thorough analysis but also get power stats over the Internet. Let\u0026rsquo;s build this ðŸ˜ƒ\nComponents SCT-013-030: A non-invasive current sensor with a clip-on. I did not wanted to meddle with 230V mains supply, hence this sensor. You can clip it on any wire and it will generate output voltage corresponding to the current flowing from the wire. Arduino Uno: I had this sitting in my closet. It has an ADC onboard which can be used to measure output of the current sensor. ESP01 WiFi module: This module can directly transmit data over WiFi network and it was the cheapest one that I found. Also, it seems it can be easily interfaced with Arduino. 3.3V power supply: ESP01 needs a regulated 3.3v power supply A raspberry pi: This is used for capturing the data shared by ESP01 and as a data logger. Eventually this will also be used as a server to broadcast captured data over Internet. A Note on ESP01 I am using Micropython firmware for ESP01. Stock AT firmware should also work. I chose Micropython because I was facing issues communicating with Arduino over serial. Although I later found out that the issue was because of power supply. ESP01 strictly needs 3.3V[1]. The issue went away once it was powered by a dedicated 3.3V power supply instead of Arduino\u0026rsquo;s 3.3V supply. I spent lot of time resolving this issue. ðŸ˜…\nInterfacing Arduino With ESP01 The interfacing here is for serial communication between Arduino and ESP01. If you want to flash or load programs on ESP01 using Arduino you will have to change wiring. Refer this note for wiring, flashing and loading programs on ESP01.\nBelow is the wiring between UNO and ESP01 for serial communication. You can choose different pins for RX \u0026amp; TX in your code.\nUNO â€” ESP8266 RX â€” 11 TX â€” 10 GND â€” GND 3.3V â€” VCC 3.3V â€” CH_PD The configuration of Arduino environment is out of scope of this article. Arduino IDE makes it very easy to load programs on Arduino.\nSerial I/O and Broadcast Serial write with Arduino\nHere\u0026rsquo;s listing of code that writes Hello from Arduino to ESP01\n#include \u0026lt;SoftwareSerial.h\u0026gt; #define RX 10 #define TX 11 SoftwareSerial esp8266(RX, TX); void setup() { esp8266.begin(115200); } void loop() { esp8266.println(\u0026#34;Hello from Arduino\u0026#34;); delay(3000); } Serial read from ESP01\nESP SoC has 2 serial I/Os. UART 1 is write only and UART 0 can do perform both read and write. That is why we have to use UART0 for communication with Arduino. But there\u0026rsquo;s a catch with UART0, it is used by micropython REPL for loading programs. The only way to communicate with Arduino over serial is to detach UART0 from REPL. Doing this would disable the functionality to load or edit programs until it is re-attached[2].\nuos.dupterm(None, 1) # detach UART from REPL. uart = UART(0, 115200) # UART 0 is available for our program if uart.any(): message = uart.readline() # read data from serial input uos.dupterm(uart, 1) # re-attach UART with REPL before end of program Broadcast over WiFi\nNow that the data is read from serial it\u0026rsquo;s time to broadcast it over WiFi using below function\ndef broadcast(message: bytes): cs = socket(AF_INET, SOCK_DGRAM) cs.sendto(message, (\u0026#39;192.168.0.255\u0026#39;, 7007)) The socket is a UDP socket as selected by SOCK_DGRAM parameter. We are choosing to send the message over broadcast address 192.168.0.255 so that we can receive it on any device on network. Listening to the message on a Linux system is as easy as running the command\n# listen continuosly for messages on udp socket port 7007 nc -kul 7007 I would recommend that you run the serial read and broadcast operation on your ESP01 before you load the actual program to confirm everything is working properly.\nMeasuring Current We can measure AC output voltage of CT sensor using ADC on Arduino. The signal first needs to be shifted above 0V in order to meet requirements of ADC. We use a voltage divider circuit as shown below to shift the analog signal by 2.5V so that its lower peak always stays above 0V. Note that our CT sensor has output of 0-1V.\nWe will be using EmonLib from Open Energy Monitor to sample and measure the RMS current. Here\u0026rsquo;s a code listing to read current from A0 pin with EmonLib.\nEnergyMonitor emon1; void setup() { Serial.begin(9600); emon1.current(1, 30); //specify pin and calibration } void loop() { double I = emon1.calcIrms(1480); //number of samples to take Serial.println(\u0026#34;cu value=\u0026#34; + String(I));\t//write data to esp in a InfluxDB line format delay(3000); } emon1.current(1,30) is used to choose ADC pin and for calibration. For the sensor I am using, it is 30. You can find out more about calibration calculation here.\nData Logging And Dashboarding For logging we are going to use timeseries database InfluxDB. InfluxDB integrates well with Grafana which we will be using for dashboarding. Both InfluxDB and Grafana will be installed on Raspberry Pi. Installation of both application is pretty straightforward. You can refer the guides included in notes below[3].\nIngesting Data With InfluxDB Line Protocol The fastest way of inserting a record in InfluxDB is through it\u0026rsquo;s CLI. Here\u0026rsquo;s an example:\n$ influxdb \u0026gt; CREATE pmon \u0026gt; USE pmon \u0026gt; cu value=2.2 \u0026gt; cu value=2.5 First we create database pmon and then insert values 2.2 and 2.5 as current(cu) in it. You can see inserted records along with timestamps using show measurements command. InfluxDB supports a lot of interfaces REST, UDP, etc,. Since we have already implemented the broadcast over UDP with ESP01 we will be using UDP interface of InfluxDB but first we need to enable UDP interface before we can start ingesting data over it. You can enable the interface in influxdb.conf. This file on exist at /etc/influxdb/influxdb.conf on raspbian. In the udp section add\n[[udp]] enabled = true bind-address = \u0026#34;0.0.0.0:7007\u0026#34;\t# port address to listen on database = \u0026#34;pmon\u0026#34; retention-policy = \u0026#34;\u0026#34; batch-size = 2\t# write to database after collecting this many measurements There are more options in the config file which you can explore as per your requirement.\nDashboard with Grafana Integrating Grafana with InfluxDB is pretty easy. Grafana can be accessed from browser for its configuration. Just point your browser to your Pi IP with 3000 port and that should open Grafana. You will have to go through some basic configuration steps and then from the settings panel you can add InfluxDB as data source. That\u0026rsquo;s it. You can start creating panels for the data that has been recorded.\nHere\u0026rsquo;s Grafana dashboard for my home setup\nCreating Grafana Panels InfluxDB queries are fairly easy to understand so I will just put the queries behind each panel that I have used\nCurrent Panel\nSELECT value FROM \u0026quot;cu\u0026quot; Power Panel\nSELECT value*252 FROM \u0026quot;cu\u0026quot;\nThe voltage at my home averages around 252V. Since, P=VxI. Power generated = 252*value. Energy Generated Hourly in Wh panel\nSELECT integral(\u0026quot;power\u0026quot;)/3600 FROM (SELECT value*252 AS power FROM \u0026quot;cu\u0026quot;) WHERE $timeFilter GROUP BY time(1h) fill(null)\nEnergy generated is the area under the power curve. Hence, the integral. We are calculating energy generated on per hour basis. Today\u0026rsquo;s generation panel\nSELECT integral(\u0026quot;value\u0026quot;)*252/3600000 FROM (SELECT value FROM \u0026quot;cu\u0026quot;) where time \u0026gt; now()-12h\nSame as energy generated calculation but here we do for last 12 hours. Monthly energy generated panel\nSame as today\u0026rsquo;s generation panel but calculated over period of 30d Carbon panels\nCarbon generated per kWh is called as carbon factor for the grid. For India, it is 0.7 and thus Energy(E)*0.7 gives us the carbon removed from atmosphere. Final Words I am very happy with this setup and it\u0026rsquo;s been very reliable so far even though I have done all of this on a bread board! The current readings are pretty accurate above 1A. There is deviation in power and energy measurements but this is because we are assuming constant voltage while in reality it fluctuates quite a bit. Maybe I will add voltage sensor to this!\nRepository Both micropython and arduino code is uploaded at https://github.com/quaintdev/EnergyMonitor\nNotes:\nThere are instances when people have ran ESP01 chip on 5V. This is most probably because of how different manufactures design their chips. The ESP8266 SoC on board ESP01 needs 3.3V. Our program is expected to communicate with Arduino continously and that is why we cannot re-attach the REPL. In the final code listing I have added delay of 10 seconds before detatching REPL. This gives us enough time to load new programs on ESP if required. If we do not do this then we loose the ability to load programs on ESP until we re-flash the firmware. Install grafana on Raspberry PI\nInstall influxdb on Raspberry PI CT sensor interfacing with Arduino ","permalink":"https://rohanrd.xyz/posts/monitoring-solar-power-over-wifi/","summary":"Measuring solar panel output with Arduino and ESP8266 and viewing it on Grafana dashboard with InfluxDB hosted on Raspberry PI","title":"Monitoring Solar Power Over WiFi"},{"content":"After taking dozens of Golang interviews over past 6 months, I have realized lot of candidates have no idea when to use or not use Go interfaces. During interviews, I asked people simple problem: there are 3 entities Student, Teacher and Employee. All these 3 entities will have a name parameter. We need a method which will print name of that entity on console. There must be only one implementation of the method and we should be able to call the method as below\nvar s Student s.PrintName() var t Teacher t.PrintName() Sounds simple, right? Surprisingly, only 2-3 candidates gave the expected answer, others chose interfaces to solve this. They would define printName() as interface method and then I would go on to ask them would it not require 3 methods each for one entity? That would confuse them and they would then come up with even bizzare solution to this problem using interfaces.\nThe solution This problem does not require interfaces at all. Consider below code\ntype People struct{ Name struct } func (p *People) PrintName(){ fmt.Println(\u0026#34;Name:\u0026#34;, p.Name) } type Student struct{ People } type Teacher struct{ People } Below code now works perfectly\nvar s Student s.PrintName() var t Teacher t.PrintName() The name parameter is common across all three entities so we define new entity called Person and embed it inside all three entities. We implement a method call PrintName() on Person struct and that method becomes available for all three entities.\nAs you can see, embedding is the fastest way to acheieve inheritance in Golang. That\u0026rsquo;s all for now people.\nTake care \u0026amp; have a great day!\n","permalink":"https://rohanrd.xyz/posts/inheritance-with-golang/","summary":"Implementing inheritance pattern in Go without using Interfaces","title":"Inheritance with Golang"}]